{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Project Assignment - Samuele Ceol\n",
    "\n",
    "\n",
    "## Classification task on heart conditions\n",
    "\n",
    "## Table of Contents\n",
    "* [CHAPTER 0 - INTRODUCTION](#0)\n",
    "    * [Section 0.1 - Project rationale & objective](#0.1)\n",
    "    * [Section 0.2 - Dataset exploration](#0.2)\n",
    "* [CHAPTER 1 - DATA CURATION](#1)\n",
    "* [CHAPTER 2 - DEVELOPMENT OF AN ALGORITHM] (#2)\n",
    "    * [Section 2.1 - Development of a decision tree algorithm] (#2.1)\n",
    "    * [Section 2.2 - Development of a random forest algorithm] (#2.2)\n",
    "* [CHAPTER 3 - TRAINING THE ALGORITHM](#3)\n",
    "* [CHAPTER 4 - COMPARING RESULTS WITH AN EXISTING SOLUTION](#4)\n",
    "* [CHAPTER 5 - CONCLUSIONS](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 0 - INTRODUCTION <a class=\"anchor\" id=\"0\"></a>\n",
    "\n",
    "## Section 0.1 - Project rationale & objective <a class=\"anchor\" id=\"0.1\"></a>\n",
    "\n",
    "The initial idea for the development of this projects came from the desire of obtaining a deeper understanding of tree-based algorithms and in how they can be utilized to construct models capable of making predictions in the context of supervised machine learning. One of the best ways to try an have an insight in the inner workings of any algorithm is to make an attempt at the development (from scratch) of a custom version of it. Because of this reason the main focus of this project will be in the development of an implementetion of the CART (Decision Tree) and Random Forest algorithms. Said implementetion will be developed in Python and a baseline will be established using comparable algorithms coming from the Sklearn library in order to assess the general quality of the custom implementetion.\n",
    "\n",
    "A dataset related to the medical domain (The Cleveland Heart Disease dataset) has been chosen to construct the machine learning models that we are going to use in this project. This particular choice aims at simulating how these types of algorithms could be employed in a real-world scenario. One of the main advantages of tree-based algorithms is their ease of understanding for individuals that are not well versed in the domain of machine learning. The medical field is a perfect example of how Decision Trees could offer an easily understandable solution for medical professionals seeking for help in spotting patients at risk of having an heart condition.\n",
    "\n",
    "## Section 0.2 - General dataset information\n",
    "\n",
    "### Creators\n",
    "\n",
    "1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n",
    "2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n",
    "3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n",
    "4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.\n",
    "\n",
    "### Donor\n",
    "\n",
    "David W. Aha (aha '@' ics.uci.edu) (714) 856-8779\n",
    "\n",
    "### Brief Description\n",
    "\n",
    "The dataset used in this project is the Cleveland Heart Disease dataset originally published in the UCI Machine Learning Repository. The dataset contains a snapshot of the clinical profile on admission to the hospital of more than 300 patients, together with a binary label indicating if an heart condition was discovered after admission.\n",
    "\n",
    "### Features\n",
    "\n",
    "- age: Age in years\n",
    "- sex: Sex\n",
    "    - Value 0: female\n",
    "    - Value 1: male\n",
    "- cp: Chest pain type\n",
    "    - Value 0: typical angina\n",
    "        - Meets all three of the following characteristics:\n",
    "            - Substernal chest discomfort of characteristic quality and duration\n",
    "            - Provoked by exertion or emotional stress\n",
    "            - Relieved by rest and/or nitroglycerine\n",
    "    - Value 1: atypical angina\n",
    "        - Meets two out of the three characteristics\n",
    "    - Value 2: non-anginal pain\n",
    "        - Meets one or none of the three characteristics\n",
    "    - Value 3: asymptomatic\n",
    "- trestbps: Resting blood pressure in mm Hg (millimiters of mercury) on admission to the hospital\n",
    "- chol: Serum cholestoral in mg/dl\n",
    "- fbs: Fasting blood sugar > 120 mg/dl\n",
    "    - Value 0 = false\n",
    "    - Value 1 = true\n",
    "- restecg: Resting electrocardiographic results\n",
    "    - Value 0: normal\n",
    "    - Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "    - Value 2: showing probable or definite left ventricular hypertrophy (left pumping chamber that has thickened and may not be pumping efficiently) by Estes' criteria\n",
    "- thalach: Maximum heart rate achieved\n",
    "- exang: Exercise induced angina\n",
    "    - Value 0 = no\n",
    "    - Value 1 = yes\n",
    "- oldpeak: ST depression induced by exercise relative to rest\n",
    "- slope: the slope of the peak exercise ST segment\n",
    "    - Value 0: upsloping\n",
    "    - Value 1: flat\n",
    "    - Value 2: downsloping\n",
    "- ca: number of major vessels colored by flourosopy\n",
    "- thal: Thalassemia (blood disorder in which the body makes an abnormal form or inadequate amount of hemoglobin)\n",
    "    - Value 0: normal\n",
    "    - Value 1: fixed defect\n",
    "    - Value 2: reversable defect\n",
    "- Condition:\n",
    "    - Value 0: heart condition not present\n",
    "    - Value 1: heart condition present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0.3 - Dataset exploration <a class=\"anchor\" id=\"0.3\"></a>\n",
    "\n",
    "As a first step, we will start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# This block contains all the libraries used in this notebook\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.ticker as mtick\n",
    "import miceforest as mf\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape & Column names\n",
    "\n",
    "Let's now proceed to import the dataset and visualize its shape in order to get a an initial overview regarding the nr of examples and features we will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/heart_cleveland_upload.csv')\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the default column names\n",
    "\n",
    "We will now rename our columns in order to make them more understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['age', 'sex', 'chest_pain', 'resting_blood_pressure', 'cholesterol_level', 'fasting_blood_sugar', 'resting_ecg', 'max_heart_rate', 'exercise_induced_angina', 'st_depression', 'st_slope', 'nr_major_vessels', 'thalassemia', 'condition']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset head\n",
    "\n",
    "It might also be useful to see how the first examples in our dataset look like. This will help us to get a first idea of the type of processing we will need to perform on the data. In addition, we will also print the data type associated with each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data types and unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might now be interesting to see what are the values associated with the subset of categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['sex', 'chest_pain', 'fasting_blood_sugar', 'resting_ecg', 'exercise_induced_angina', 'st_slope', 'nr_major_vessels', 'thalassemia']:\n",
    "    print(f\"{i}: {df[i].dropna().unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "\n",
    "Another important information about our dataset that we will need to consider when curating our data is the presence of null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, only the `nr_major_vessels` and `thalassemia` contains examples with missing values. \n",
    "\n",
    "### Feature correlation\n",
    "\n",
    "As a last step in this initial exploration, we will look at the level of correlation between the various features in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "plt.title('Feature correlation')\n",
    "\n",
    "df_corr = df.corr()\n",
    "mask = np.zeros_like(df_corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "sns.heatmap(\n",
    "    df_corr, \n",
    "    linecolor='black', \n",
    "    mask=mask,\n",
    "    annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0.4 - Data distribution <a class=\"anchor\" id=\"0.3\"></a>\n",
    "\n",
    "Before moving to the data curation phase, we will take a quick look at how the values for all features are distributed in our dataset\n",
    "\n",
    "### Target class distribution\n",
    "\n",
    "We will now look at the distribution of heart conditions in our dataset. Knowing how balanced our data is in relation to the `condition` feature is particularly important since we want to ultimately be able to train models capable of predicting the value of this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_proportion(curr_df, title):\n",
    "    g = sns.barplot(\n",
    "        x='condition',\n",
    "        y='condition',  \n",
    "        alpha=.75,\n",
    "        palette=['#3697b5', '#d11313'],\n",
    "        edgecolor='black',\n",
    "        linewidth=0,\n",
    "        data=curr_df,\n",
    "        estimator=lambda x: len(x) / len(curr_df) * 100\n",
    "    )\n",
    "    \n",
    "    g.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "    g.set_ylabel('')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "condition_proportion(df, \"Proportion of heart conditions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the dataset that we are using is slighlty unbalanced. Even though the discrepancy between the two classes is not particularly high, we will still employ some oversampling techniques for the minority class on part of our data in order to successfully train our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the data is unbalanced, divide it for representation purposes\n",
    "split_df = df.copy()\n",
    "\n",
    "split_df = split_df[split_df['nr_major_vessels'].notna()]\n",
    "split_df = split_df[split_df['thalassemia'].notna()]\n",
    "\n",
    "split_df.nr_major_vessels = split_df.nr_major_vessels.astype(int)\n",
    "split_df.thalassemia = split_df.thalassemia.astype(int)\n",
    "\n",
    "condition_df = split_df[split_df['condition'] == 1]\n",
    "no_condition_df = split_df[split_df['condition'] == 0]\n",
    "\n",
    "numeric_features = ['age', 'resting_blood_pressure', 'cholesterol_level', 'max_heart_rate', 'st_depression']\n",
    "subplot = 231\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i in numeric_features:\n",
    "    plt.subplot(subplot)\n",
    "\n",
    "    sns.kdeplot(\n",
    "        x=i,\n",
    "        alpha=.75,\n",
    "        color='#d11313', #Red\n",
    "        shade=True,\n",
    "        linewidth=0,\n",
    "        data=condition_df\n",
    "    )\n",
    "\n",
    "    sns.kdeplot(\n",
    "        x=i,\n",
    "        alpha=.75,\n",
    "        color='#3697b5', #Blue\n",
    "        shade=True,\n",
    "        linewidth=0,\n",
    "        data=no_condition_df\n",
    "    )\n",
    "\n",
    "    subplot += 1\n",
    "\n",
    "plt.suptitle('Distribution of numerical data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['sex', 'chest_pain', 'fasting_blood_sugar', 'resting_ecg', 'exercise_induced_angina', 'st_slope', 'thalassemia', 'nr_major_vessels']\n",
    "\n",
    "categorical_values = [['Female', 'Male'], ['typical\\nangina', 'atypical\\nangina', 'non-anginal\\npain', 'asymptomatic'], ['False', 'True'], ['Normal', 'ST-T\\nwave\\nabnormality', 'Left\\nVentricular\\nHypertrophy'], ['No', 'Yes'], ['upsloping', 'flat', 'downsloping'], ['normal', 'fixed defect', 'reversable defect'], ['0', '1', '2', '3']]\n",
    "\n",
    "fig, ax = plt.subplots(len(categorical_features), 2, figsize=(10, 25), sharey=True)\n",
    "\n",
    "for i in range(len(categorical_features)):\n",
    "    feature = categorical_features[i]\n",
    "    values = categorical_values[i]\n",
    "\n",
    "    sns.barplot(\n",
    "        x=feature, \n",
    "        y=feature,\n",
    "        alpha=.75,\n",
    "        color='#3697b5',\n",
    "        data=no_condition_df, \n",
    "        estimator=lambda x: len(x) / len(no_condition_df) * 100,\n",
    "        ax=ax[i,0]\n",
    "    ).set_xticklabels(values)\n",
    "\n",
    "    if(feature == 'work_type'):\n",
    "        values.remove('Never worked')\n",
    "\n",
    "    sns.barplot(\n",
    "        x=feature, \n",
    "        y=feature,\n",
    "        alpha=.75,\n",
    "        color='#d11313',\n",
    "        data=condition_df, \n",
    "        estimator=lambda x: len(x) / len(condition_df) * 100,\n",
    "        ax=ax[i,1]\n",
    "    ).set_xticklabels(values)\n",
    "\n",
    "    ax[i,0].set_title(feature)\n",
    "    ax[i,0].set_ylabel('')\n",
    "    ax[i,1].set_ylabel('')\n",
    "    ax[i,0].set_xlabel('')\n",
    "    ax[i,1].set_xlabel('')\n",
    "    ax[i,0].yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.suptitle('Distribution of categorical data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 1 - DATA CURATION <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "We will now start with the process of preparing our dataset for the machine learning algorithms we will later use. We will start off this process by handling the missing values.\n",
    "\n",
    "### Handling missing values\n",
    "\n",
    "As we have seen at the beginning of this project, some examples are missing the value for the features `nr_major_vessels` and `thalassemia`. The easiest way to solve this problem would be to simply drop the interested rows or to substitute the missing values with the mean of the `nr_major_vessels` and `thalassemia` features (mean imputation). Mean imputation is a type of univariate imputation, because it only takes into account the feature that we are trying to fill in order to come up with the missing data. A more advanced approach to this issue is to use a multivariate imputation algorithm, which instead will take into consideration the whole set of features to perform the estimation.\n",
    "\n",
    "We will use an algorithm called Multivariate Imputation by Chained Equation (MICE) in order to complete this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = mf.KernelDataSet(df, save_all_iterations=True, random_state=42)\n",
    "kernel.mice(5) # Nr of iterations\n",
    "df = kernel.complete_data()\n",
    "\n",
    "df.nr_major_vessels = df.nr_major_vessels.astype(int)\n",
    "df.thalassemia = df.thalassemia.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling non-binary categorical data\n",
    "\n",
    "At this point, we are left with three categorical features containing binary data (`sex`, `fasting_blood_sugar` and `exercise_induced_angina`) and five features containing categorical data with a cardinality bigger than two (`chest_pain`, `resting_ecg`, `st_slope`, `nr_major_vessels` and `thalassemia`).\n",
    "\n",
    "During the development of our Decision Tree and Random Forest algorithms, we will work with binary trees. Binary trees are able to handle binary categorical data (values 0 and 1) without any particular issue.\n",
    "\n",
    "When a feature has a cardinality bigger than two, encoding it as an integer will impose an order on its values. This would be correct when working with ordinal categorical values (e.g low/medium/high becomes 0/1/2), but fails completely in our case since no order can be established between the categories of our features.\n",
    "\n",
    "Because of this reason we will further process these two features in order to make them more suitable to use in our machine learning algorithms.\n",
    "\n",
    "We will approach this issue by using One-hot encoding. This technique will effectively turn a single categorical feature into multiple binary ones. The number of newly created (dummy) binary features corresponds to the number of categories in the original feature. \n",
    "\n",
    "When using One-hot encoding in the context of tree-based algorithms we have to consider that:\n",
    "\n",
    ">One-hot encoding categorical variables with high cardinality can cause inefficiency in tree-based ensembles. Continuous variables will be given more importance than the dummy variables by the algorithm which will obscure the order of feature importance resulting in poorer performance.\n",
    "\n",
    "- One-Hot Encoding is making your Tree-Based Ensembles worse, hereâ€™s why. Rakesh Ravi\n",
    "\n",
    "Since the cardinalities of the features we are working with are fairly low (<=4), this issue will more than likely not affect us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_cardinality_cols = ['chest_pain', 'resting_ecg', 'st_slope', 'nr_major_vessels', 'thalassemia']\n",
    "df = pd.get_dummies(df, columns=high_cardinality_cols, drop_first=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way, Target Encoding\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from category_encoders import TargetEncoder\n",
    "\n",
    "high_cardinality_cols = ['chest_pain', 'resting_ecg', 'st_slope', 'nr_major_vessels', 'thalassemia']\n",
    "encoder = TargetEncoder()\n",
    "\n",
    "for column in high_cardinality_cols:\n",
    "    df[column] = encoder.fit_transform(df[column], df['condition'])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling (Standardizing continuous data)\n",
    "\n",
    "Some machine learning algorithms are highly sensitive to differing degrees of magnitudes between features. Said algorithms (like KNN, K-Means and SVM) will tend to give an higher importance towards features having an higher magnitude.\n",
    "There are two approaches to address this problem: normalization and standardization.\n",
    "\n",
    "Normalization is a technique in which values are rescaled so that they end up in a range between 0 and 1. The formula for feature normalization is as follows:\n",
    "\n",
    "$$ X' = \\frac{X - X_{min}}{X_{max} - X_{min}} $$\n",
    "\n",
    "On the other hand, standardization (or Z-score normalization) rescales the values for a given feature such that it will have the properties of standard normal distribution around the mean (that takes value 0) and the standard deviation (that takes value 1). The formula for standardization is as follows:\n",
    "\n",
    "$$ X' = \\frac{X-\\mu}{\\sigma} $$\n",
    "\n",
    "In the context of this project, feature scaling is a completely optional step. Tree-based algorithm are not touched by this issue since they decide upon a split by looking at features \"in isolation\". Because of this fact, we will obtain the same results even without running the following code block.\n",
    "\n",
    "It was deemed appropiate to perform this step in order to prepare the data for future work related to this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling (Standardization) - Removing the median and scaling the data according to the quantile range \n",
    "cols_to_scale = ['age', 'resting_blood_pressure', 'cholesterol_level', 'max_heart_rate', 'st_depression']\n",
    "ss = StandardScaler()\n",
    "df[cols_to_scale] = ss.fit_transform(df[cols_to_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Test Split\n",
    "\n",
    "Before addressing the issue related to the slight unbalance in our dataset, we will first create our training and testing subsets. It is important to remember that the oversampling technique perfomed in the next subsection is only applied to the training set.\n",
    "\n",
    "The parameter `stratify` set to our target variable will make sure that the testing split will have the same ratio of the target class that we have found in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['age', 'sex', 'resting_blood_pressure', 'cholesterol_level',\n",
    "       'fasting_blood_sugar', 'max_heart_rate', 'exercise_induced_angina',\n",
    "       'st_depression', 'chest_pain_1', 'chest_pain_2',\n",
    "       'chest_pain_3', 'resting_ecg_1', 'resting_ecg_2', 'st_slope_1',\n",
    "       'st_slope_2', 'nr_major_vessels_1', 'nr_major_vessels_2',\n",
    "       'nr_major_vessels_3', 'thalassemia_1', 'thalassemia_2']]\n",
    "y = df[['condition']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=df['condition'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling unbalanced data (oversampling with SMOTE)\n",
    "\n",
    "As already seen at the beginning of this project, the dataset we are working with is slightly unbalanced. This is a fairly common occurence when working with medical data, and in our case it means that the `condition` attribute of our training examples will be overrepresented by a majority class (0 in our case) that will cause our model to be slightly biased towards it (i.e. in more extreme cases the model would tend to see the minority class as data noise). \n",
    "\n",
    "A technique that we can use to address this problem is to oversample our minority class by synthesizing new (training set) entries. The simplest way to implement oversampling would be to randomly duplicate existing instances of the minority class. A more interesting way to solve this problem is to use SMOTE (Synthetic Minority Oversampling Technique). Instead of simply duplicating existing data objects, SMOTE will select an object from the minority class. The algorithm will then select another object from the pool of its k-nearest neighbors. A new synthetic example is then created at a random point between the two selected examples. The process is repeated until the classes in the dataset have been balanced.\n",
    "\n",
    "<img src=\"./images/SMOTE.png\" alt=\"k-fold\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE(random_state=42)\n",
    "X_train_SMOTE, y_train_SMOTE = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "oversampled_df = X_train_SMOTE.assign(condition = y_train_SMOTE)\n",
    "condition_proportion(y_train_SMOTE, \"Proportion of heart conditions (oversampled training data)\")\n",
    "print(\"Shape before SMOTE: \" + str(X_train.shape))\n",
    "print(\"Shape after SMOTE: \" + str(X_train_SMOTE.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 2 - DEVELOPMENT OF AN ALGORITHM <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "## Section 2.1 - Development of a decision tree algorithm <a class=\"anchor\" id=\"2.1\"></a>\n",
    "\n",
    "The algorithm that has been chosen to be implemented for the classification task of this projects is called CART (short for Classification and Regression Trees). An improved version of this algorithm is used by the `DecisionTreeClassifier` and `RandomForestClassifier` of the `sklearn.tree` library.\n",
    "\n",
    "### CART (Classification and Regression Trees)\n",
    "\n",
    "### Building the tree\n",
    "\n",
    "CART is an algorithm that recursively builds binary trees (branching factor for root and inner nodes = 2) by searching in a partition the combination of feature and threshold values that will maximize the reduction in impurity obtained by splitting a given partition using a selected split candidate. Like many tree-based algorithms (like ID3, C4.5), CART builds the tree structure from the top-down by starting from a `root` partition which containins all the training data. This recursive split (from parent partition to left and right child nodes) is what ultimately allows the algorithm to build the tree structure. \n",
    "\n",
    "### Searching for a split candidate\n",
    "\n",
    "For each newly created parittion, CART will look at each combination of feature and threshold in order to find a split candidate. For every feature containing `K` different values, we will always have `K-1` possible thresholds. It can also be decided (by changing the value of `max_features`) to search for a split only in a subset of randomly sampled features, this is the default behavior when a tree is initialized from the `RandomForestClassifier` (where `max_features = sqrt`).\n",
    "\n",
    "Before searching for a possible split, the implemented algorithm will (as a first step) sort the partition entries based on the values of the selected feature. Sorting is fairly expensive, but it allows the algorithm to easily search for all thresholds (within a given feature) in linear time. The algorithm will then proceed to traverse the list of sorted examples and to evaluate all midpoints between two (different) adjecent values as a possible threshold. The reduction in impurity that would be obtained by splitting at the currently selected threshold is compared with the maximum reduction that has been registered so far for the current partition. If the current reduction is bigger than the previous best one, the details of the best split candidate are updated.\n",
    "\n",
    "### Stopping condition\n",
    "\n",
    "In CART, the splitting process is recursively repeated for each newly created node until a stopping condition is met. The decision to stop the splitting process for a given node is triggered when: \n",
    "- The partition is pure (i.e. all the examples in the partition have the same class)\n",
    "- No possible split can be imposed to the current partition (i.e. All examples only have a single value per feature)\n",
    "- The (user defined) maximum depth (`max_depth`) has been reached\n",
    "- The threshold for the reduction is impurity (`min_impurity_decrease`) cannot be satisfied by any split candidate\n",
    "- The threshold for the minimum nr of examples needed to split a partition (`min_samples_split`) is not met\n",
    "- The impurity for the current partition is below a given threshold (`min_impurity_split`) \n",
    "- The current partition cannot produce two children with the required minimum nr of entries (`min_samples_leaf`)\n",
    "\n",
    "Whenever a stopping condition is reached, the interested partition is turned into a leaf node. Majority voting is performed in order to decide what is the predicted value of the newly created leaf.\n",
    "\n",
    "### Split strategy\n",
    "\n",
    "For this algorithms implementation two strategies have been offered to asses the quality of a split candidate (feature+threshold): `gini` and `entropy`\n",
    "\n",
    "Initializing our `CustomDecisionTreeClassifier` with `criterion = gini` will cause our algorithm to use the Gini Index as a starting metric to evaluate the reduction in impurity brought upon by a given split candidate. \n",
    "\n",
    "The Gini Index for a partition D containing examples with C different classes is calculated as follows:\n",
    "\n",
    "$$ Gini(D) = 1 - \\sum \\limits _{i=1} ^{C} (p _{i}) ^{2} $$\n",
    "\n",
    "When considering a split candidate A (feature+threshold) for a partition D, we calculate the weighted sum of impurities of the resulting child partitions D1 and D2 as follows:\n",
    "\n",
    "$$ Gini _{A} (D) = \\frac{size(D _{1})}{size(D)} Gini(D _{1}) + \\frac{size(D _{2})}{size(D)} Gini(D _{2})$$\n",
    "\n",
    "The reduction of impurity is then simply:\n",
    "\n",
    "$$ \\Delta Gini(A) = Gini(D) - Gini _{A} (D) $$\n",
    "\n",
    "Alternatively, the algorithm can be initialized with `criterion = entropy` to use the Information gain as a metric for the reduction in impurity.\n",
    "\n",
    "The entropy of a partition D containing examples with C different classes is calculated as follows:\n",
    "\n",
    "$$ Entropy(D) = - \\sum \\limits _{i=1} ^{C} p _{i} log _{2} (p _{i}) $$\n",
    "\n",
    "The weighted sum of entropies for child partitions D1 and D2 originating from D via a split candidate A is:\n",
    "\n",
    "$$ Entropy _{A} (D) = \\frac{size(D _{1})}{size(D)} Entropy(D _{1}) + \\frac{size(D _{2})}{size(D)} Entropy(D _{2})$$\n",
    "\n",
    "The Information Gain is then simply:\n",
    "\n",
    "$$ Gain(A) = Entropy(D) - Entropy _{A} (D) $$\n",
    "\n",
    "### Pruning\n",
    "\n",
    "Often times when working with tree based algorithms we end up constructing trees with some branches reflecting anomalies (e.g. outliers) of the data that was used to construct them. This leads to overfitting and hinders the capacity of the model to generalize well to new data. In order to tackle this issue we will use a techingue called pruning. Pruning entails cutting out certain branches of the tree in order to reduce the model size and improve its performance. We distinguish two types of pruning: pre-pruning and post-pruning.\n",
    "\n",
    "Pre-pruning (or forward pruning or early stopping) is a type of pruning which prevents the creation of non-significant branches. Pre-pruning is achieved by tuning parameters such as `max_depth`, `min_impurity_decrease`, `min_impurity_split`, `min_samples_split` and `min_samples_leaf` to avoid splits that would otherwise occur during the creation of the tree.\n",
    "\n",
    "Post-pruning is a type of pruning that removes sub-trees (by replacing them with leaf nodes) once they have already been grown. A popular algorithm used for post pruning is the Cost Complexity Pruning algorithm. Due to time constraints, CCP has not been implemented in our custom implementation of the CART algorithm. A section showing how the Sklearn implementation of CCP can be used to improve the performance of a tree-based model can be seen at the end of this project.\n",
    "\n",
    "### Partition class\n",
    "\n",
    "To represent the structure of a node in our Decision Tree, it has been decided to create a `Partition` class that will contain all the information related to a given node.\n",
    "Root and inner nodes will contain information regarding the chosen split candidate (`split_feature`, `split_value`), the level of impurity (`impurity_part`), the reduction of impurity that has been obtained with the chosen split candidate (`impurity_delta`) and the weighted sum of impurities of the child nodes obtained with the chosen split candidate (`impurity_feature_example`).\n",
    "Additionally, the non-leaf nodes also have populated pointers to their child partitions (`left_part` and `right_part`). This characteristics of the partition class is what allows us to traverse the full-length of the tree by starting from the root node and jumping from child to child until a leaf node is reached.\n",
    "\n",
    "For both root/inner nodes and leaf nodes we have information regarding the predicted class values (`prediction`) which is obtained by majority voting, and the class distribution (i.e. the number of examples for each class value, represented by `counter`).\n",
    "\n",
    "### Predicting a class\n",
    "\n",
    "Predicting a class value from a trained model is simply a matter of traversing the three from root to leaf. Non-leaf nodes contain information regarding the feature (`split_feature`) and threshold (`split_value`) we have to check in order to decide if we want to move to the left (example value <= partition threshold) or right (example value > partition threshold) child from the current partition. Once a leaf node is reached, the algorithm stops and checks the predicted value (`prediction`) in order to decide what the predicted class value will be for the current example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaf partitions only have prediction values\n",
    "class Partition:\n",
    "    '''\n",
    "    Class used to represent a partition\n",
    "    Partitions act as nodes in the Decision Tree\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    split_feature: str\n",
    "        Feature chosen for splitting the partition\n",
    "    split_value: float\n",
    "        (Threshold) Value chosen for splitting the partition\n",
    "    impurity_part: float\n",
    "        Impurity of the current partition\n",
    "    impurity_delta: float\n",
    "        Reduction in impurity by splitting using `split_feature` and `split_value`\n",
    "    impurity_feature_example: float\n",
    "        Weighted sum of impurities for the partitions resulting from a split at `split_feature` and `split_value`\n",
    "    prediction: float\n",
    "        Predicted value (majority class) at the current partition\n",
    "    counter: collections.Counter\n",
    "        Counter for each class in the parition\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 split_feature, \n",
    "                 split_value, \n",
    "                 impurity_part,\n",
    "                 impurity_delta, \n",
    "                 impurity_feature_example, \n",
    "                 prediction,\n",
    "                 counter):\n",
    "        self.split_feature = split_feature\n",
    "        self.split_value = split_value\n",
    "        self.impurity_part = impurity_part\n",
    "        self.impurity_delta = impurity_delta\n",
    "        self.impurity_feature_example = impurity_feature_example\n",
    "        self.prediction = prediction\n",
    "        self.counter = counter\n",
    "        self.left_part = None\n",
    "        self.right_part = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDecisionTreeClassifier:\n",
    "    '''\n",
    "    Class used to model the decision tree\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    criterion: str\n",
    "        Strategy used to calculate the impurity of a partition\n",
    "        'gini' (for gini index) or 'entropy' (for information gain)\n",
    "    max_depth: int\n",
    "        Maximum total depth of the tree. Root node is at depth 1\n",
    "    min_impurity_decrease: float\n",
    "        Threshold for the reduction in impurity needed to split a partition\n",
    "        If the values is not met, the interested partition is turned into a leaf node\n",
    "    min_impurity_split: float\n",
    "        Impurity threshold below which a partition is turned into a leaf node \n",
    "    min_samples_split: int\n",
    "        Threshold for the nr of examples needed to split a partition\n",
    "        If the values is not met, the interested partition is turned into a leaf node\n",
    "    max_features: str, int\n",
    "        The number of features (randomly) considered at each node in order to find a split point\n",
    "        Note: \n",
    "            If no valid split point can be found with the initially sampled features, before turning the parition into \n",
    "            a leaf node the system will try to sample additional features until at least one combination of \n",
    "            'split_feature' and 'split_value' is found or until all possible features are exhausted\n",
    "        Can be manually set (int) or as follows (str):\n",
    "            'None' considers all the features of in training set\n",
    "            'sqrt' considers the square root of the total nr of features in the training set\n",
    "            'log2' considers the log base 2 of the total nr of features in the training set\n",
    "    min_samples_leaf: int\n",
    "        The minimum nr of entries required for a leaf node\n",
    "        A split candidate is considered only if it creates two child nodes that respect this parameter\n",
    "    '''\n",
    "    def __init__(self, *,\n",
    "                 criterion = 'gini',\n",
    "                 max_depth = None, \n",
    "                 min_impurity_decrease = 0.0,\n",
    "                 min_impurity_split = None,\n",
    "                 min_samples_split = 2,\n",
    "                 max_features = None,\n",
    "                 min_samples_leaf = 1):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth # Root depth is 0\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.min_impurity_split = min_impurity_split\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.n_classes = 0\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, \n",
    "            X, \n",
    "            y):\n",
    "        '''\n",
    "        Starts the process of building a Decision Tree from a training set\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: pandas.core.frame.DataFrame, numpy.ndarray\n",
    "            The training examples\n",
    "        y: pandas.core.frame.DataFrame, numpy.ndarray\n",
    "            The training class labels\n",
    "        '''\n",
    "        X, y = self.data_to_numpy(X,y)\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.root = self.build_tree(X, y, self.max_depth)\n",
    "\n",
    "    @staticmethod\n",
    "    def data_to_numpy(X,\n",
    "                      y = None):\n",
    "        '''\n",
    "        Converts input dataframes to numpy arrays\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: pandas.core.frame.DataFrame, numpy.ndarray\n",
    "            The training examples\n",
    "        y: pandas.core.frame.DataFrame, numpy.ndarray\n",
    "            The training class labels\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            The training examples converted to a numpy array\n",
    "        y: numpy.ndarray\n",
    "            The training class labels converted to a (flattened) numpy array\n",
    "        '''\n",
    "        if(type(X) != np.ndarray):\n",
    "            X = X.to_numpy()\n",
    "        if((y is not None) and (type(y) != np.ndarray)):\n",
    "            y = y.to_numpy()\n",
    "\n",
    "        if(y is not None):\n",
    "            return X, y.flatten()\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def build_tree(self, \n",
    "                   X, \n",
    "                   y, \n",
    "                   depth):\n",
    "        '''\n",
    "        Recursively builds a Decision Tree starting from a training set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: pandas.core.frame.DataFrame\n",
    "            The training examples\n",
    "        y: numpy.ndarray\n",
    "            The training class labels\n",
    "        depth: int\n",
    "            Depth counter\n",
    "            Used as stop condition for the recursive call when `max_depth` is defined for the Decision Tree\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        curr_part: Partition\n",
    "            The root partition (node) of the Decision Tree\n",
    "        '''\n",
    "        # Majority class is set as the predicted value for the current partition\n",
    "        counter = Counter(y)\n",
    "        prediction = counter.most_common(1)[0][0]\n",
    "\n",
    "        # Update depth value\n",
    "        if self.max_depth:\n",
    "            if depth > 0:\n",
    "                depth -= 1 \n",
    "            else:\n",
    "                return Partition(None, None, None, None, None, prediction, counter) # Return as leaf\n",
    "\n",
    "        # Check partition size\n",
    "        if y.size < self.min_samples_split:\n",
    "            return Partition(None, None, None, None, None, prediction, counter) # Return as leaf\n",
    "\n",
    "        split_feature, split_value, impurity_part, impurity_delta, impurity_feature_example = self.find_split(X, y)\n",
    "        curr_part = Partition(split_feature, split_value, impurity_part, impurity_delta, impurity_feature_example, prediction, counter)\n",
    "\n",
    "        if(split_feature is not None):\n",
    "            # Temporarely create a merged df in order to filter features and classes together\n",
    "            merged_df = np.append(X, np.vstack(y), axis=1)\n",
    "            \n",
    "            merged_df_left = merged_df[merged_df[:,split_feature] < split_value]\n",
    "            merged_df_right = merged_df[merged_df[:,split_feature] > split_value]\n",
    "\n",
    "            curr_part.left_part = self.build_tree(\n",
    "                merged_df_left[:, :-1], \n",
    "                merged_df_left[:, -1], \n",
    "                depth) # Left\n",
    "\n",
    "            curr_part.right_part = self.build_tree(\n",
    "                merged_df_right[:, :-1], \n",
    "                merged_df_right[:, -1], \n",
    "                depth) # Right  \n",
    "\n",
    "        return curr_part\n",
    "\n",
    "    def predict(self, \n",
    "                X):\n",
    "        '''\n",
    "        Predicts the class value (label) for each input example\n",
    "        The tree is traversed for each provided example\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.ndarray, pandas.core.frame.DataFrame\n",
    "            The examples to predict the class for\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        y: numpy.ndarray\n",
    "            The predicted class labels\n",
    "        '''\n",
    "        X = self.data_to_numpy(X)\n",
    "        y = []\n",
    "\n",
    "        for example in X:\n",
    "            curr_part = self.root\n",
    "\n",
    "            while curr_part.right_part:\n",
    "                if example[curr_part.split_feature] <= curr_part.split_value:\n",
    "                    curr_part = curr_part.left_part\n",
    "                else:\n",
    "                    curr_part = curr_part.right_part\n",
    "            y.append(curr_part.prediction)\n",
    "\n",
    "        return np.array(y)\n",
    "\n",
    "\n",
    "    def predict_proba(self, \n",
    "                      X):\n",
    "        '''\n",
    "        Predicts the probability of each class value (label) for each input example\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.ndarray, pandas.core.frame.DataFrame\n",
    "            The examples to predict the class for\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        y: numpy.ndarray\n",
    "            The probabilities for each class label\n",
    "        '''\n",
    "        X = self.data_to_numpy(X)\n",
    "        y = []\n",
    "\n",
    "        for example in X:\n",
    "            curr_part = self.root\n",
    "\n",
    "            while curr_part.right_part:\n",
    "                if example[curr_part.split_feature] <= curr_part.split_value:\n",
    "                    curr_part = curr_part.left_part\n",
    "                else:\n",
    "                    curr_part = curr_part.right_part\n",
    "\n",
    "            temp = []\n",
    "\n",
    "            for i in range(self.n_classes):\n",
    "                if(i == curr_part.prediction):\n",
    "                    temp.append(1.0)\n",
    "                else:\n",
    "                    temp.append(0.0)\n",
    "\n",
    "            y.append(temp)\n",
    "\n",
    "        return np.array(y)\n",
    "\n",
    "    # Both gini index and entropy/information gain are measures of the impurity of a node.\n",
    "    def calc_impurity(self, \n",
    "                      class_occ, \n",
    "                      size_part):\n",
    "        '''\n",
    "        Calculates the impurity of a partition based on the chosen strategy\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        class_occ: Counter\n",
    "            Nr of occurrences of each class value \n",
    "        size_part:\n",
    "            Nr of examples in the current partition\n",
    "        Returns\n",
    "        ----------\n",
    "        ret:\n",
    "            The impurity value of the partition\n",
    "        '''\n",
    "        if self.criterion == 'gini':\n",
    "            return (1 - sum((class_occ[curr_class]/size_part)**2 for curr_class in class_occ))\n",
    "        elif self.criterion == 'entropy':\n",
    "            ret = 0\n",
    "            for curr_class in class_occ:\n",
    "                if class_occ[curr_class]/size_part > 0:\n",
    "                    ret -= class_occ[curr_class]/size_part * math.log(class_occ[curr_class]/size_part, 2)\n",
    "            return ret\n",
    "\n",
    "    def find_split(self, \n",
    "                    X, \n",
    "                    y):\n",
    "            '''\n",
    "            Finds the best split feature and threshold for a given partition\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            X: pandas.core.frame.DataFrame\n",
    "                The training examples\n",
    "            y: pandas.core.frame.DataFrame\n",
    "                The training class labels\n",
    "\n",
    "            Returns\n",
    "            ----------\n",
    "            split_feature: str\n",
    "                Feature chosen for splitting the partition\n",
    "            split_value: float\n",
    "                (Threshold) Value chosen for splitting the partition\n",
    "            impurity_delta: float\n",
    "                Reduction in impurity by splitting using `split_feature` and `split_value`\n",
    "            impurity_feature_example: float\n",
    "                Weighted sum of impurities for the partitions resulting from a split at `split_feature` and `split_value`\n",
    "            '''\n",
    "            # Initialize the return values\n",
    "            split_feature, split_value, impurity_part, impurity_delta, impurity_feature_example = None, None, None, None, None\n",
    "            \n",
    "            # Calculate the occurences of each class in the partition\n",
    "            class_occ_part = Counter(y)\n",
    "\n",
    "            # Return if the partition is pure\n",
    "            if(sum(class_occ_part[i] > 0 for i in class_occ_part) == 1):\n",
    "                return split_feature, split_value, impurity_part, impurity_delta, impurity_feature_example\n",
    "\n",
    "            # Calculate the impurity of the current partition\n",
    "            size_part = y.size\n",
    "            impurity_part = self.calc_impurity(class_occ_part, size_part)\n",
    "            impurity_delta = 0\n",
    "            \n",
    "            # Return if below impurity threshold\n",
    "            if((self.min_impurity_split is not None) and (impurity_part < self.min_impurity_split)):\n",
    "                return split_feature, split_value, impurity_part, impurity_delta, impurity_feature_example\n",
    "\n",
    "            # Sample the features to consider for the split\n",
    "            X_rows, X_cols = X.shape\n",
    "            n = X_cols\n",
    "\n",
    "            if(self.max_features == 'sqrt'):\n",
    "                n=round(math.sqrt(X_cols))\n",
    "            elif(self.max_features == 'log2'):\n",
    "                n=round(math.log2(X_cols))\n",
    "            elif((self.max_features is not None) and (self.max_features <= X_cols)):\n",
    "                n=self.max_features\n",
    "\n",
    "            sampled_features = sorted(random.sample(range(0, X_cols), n))\n",
    "\n",
    "            # Loop through all features\n",
    "            for feature in sampled_features:\n",
    "                # Skip ahead if the feature has only one value\n",
    "                if len(np.unique(X[:,feature])) == 1:\n",
    "                    continue\n",
    "\n",
    "                # Sort the examples in increasing order for the selected feature\n",
    "                # Consider the midpoint between two (different) adjecent values as a possible split point\n",
    "                feature_sorted, y_sorted = zip(*sorted(zip(X[:,feature], y), key=itemgetter(0)))\n",
    "\n",
    "                # Go through the sorted feature while keeping track of class occurences on each side (/partition)\n",
    "                class_occ_left = Counter()\n",
    "                for i in range(self.n_classes):\n",
    "                    class_occ_left[i] = 0 # Left partition starts as empty (all class counters are set to 0)\n",
    "\n",
    "                class_occ_right = class_occ_part.copy() # Same as occurences of each class in the partition\n",
    "\n",
    "                for example in range(1, size_part):\n",
    "                    example_class = y_sorted[example-1]\n",
    "\n",
    "                    # Increment the example class on the left, decrement it on the right\n",
    "                    class_occ_left[example_class] += 1\n",
    "                    class_occ_right[example_class] -= 1\n",
    "\n",
    "                    # Skip ahead if two adjacent values are equal\n",
    "                    if feature_sorted[example] == feature_sorted[example - 1]:\n",
    "                        continue\n",
    "\n",
    "                    size_left = example\n",
    "                    size_right = size_part - example\n",
    "\n",
    "                    # Skip if the threshold does not respect the min_samples_leaf parameter\n",
    "                    if(size_left < self.min_samples_leaf or size_right < self.min_samples_leaf):\n",
    "                        continue\n",
    "\n",
    "                    # Calculate the impurity of each side\n",
    "                    impurity_left = self.calc_impurity(class_occ_left, size_left)\n",
    "                    impurity_right = self.calc_impurity(class_occ_right, size_right)\n",
    "                    \n",
    "                    # Weighted sum of impurities (with selected feature at current example)\n",
    "                    curr_impurity_feature_example = ((size_left/size_part) * impurity_left) + ((size_right/size_part) * impurity_right)\n",
    "\n",
    "                    # Reduction in impurity with current split\n",
    "                    curr_impurity_delta = impurity_part - curr_impurity_feature_example \n",
    "\n",
    "                    if(self.min_impurity_decrease and curr_impurity_delta < self.min_impurity_decrease):\n",
    "                        continue\n",
    "\n",
    "                    if(curr_impurity_delta > impurity_delta):\n",
    "                        split_feature = feature\n",
    "                        split_value = (feature_sorted[example] + feature_sorted[example-1]) / 2\n",
    "                        impurity_delta = curr_impurity_delta\n",
    "                        impurity_feature_example = curr_impurity_feature_example\n",
    "\n",
    "                # If no split point was found when using a random subset of features, try to randomly sample a new one\n",
    "                if((len(sampled_features) != X_cols) and (split_feature is None) and (feature == sampled_features[-1])):\n",
    "                    sampled_features.append(random.sample(set(range(0, X_cols)) - set(sampled_features), 1)[0])\n",
    "\n",
    "            return split_feature, split_value, impurity_part, impurity_delta, impurity_feature_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2 - Development of a random forest algorithm <a class=\"anchor\" id=\"2.2\"></a>\n",
    "\n",
    "Now that we have a way to build binary tree using our implementation of the CART algorithm, we are ready to move a step further by implementing a Random Forest classifier (`CustomRandomForestClassifier`) that will use our `CustomDecisionTreeClassifier` as the basic building block to perform ensemble learning.\n",
    "\n",
    "### Building a forest\n",
    "\n",
    "Building a forest is a fairly straightforward process in which a set `CustomDecisionTreeClassifier` equal to the value defined by the `n_estimators` parameter are initialized. Using the same input data for all trees and considering the same features for all node splits would create a set of identical models. Since an ensemble of nearly identical trees is fairly useless, Random Forest uses two techniques to ensure that all the models built as part of the forest are slightly different from one another:\n",
    "\n",
    "- Feature randomness\n",
    "- Bootstrap Aggregation (Bagging)\n",
    "\n",
    "### Feature randomness\n",
    "\n",
    "As we have seen in the previous section, setting the `max_features` variable of the `CustomDecisionTreeClassifier` influences how many features are randomly sampled when searching for a potential split in a partition. This particular parameter will allow the algorithm to increment the range of diversity between the trees that are part of the forest.\n",
    "\n",
    "This random sampling of features is done at the node level, NOT at the tree level. Early implementations of the random forest algorithms used a technique called â€œrandom subspace methodâ€ where the random subset of features was provided at the tree level:\n",
    "\n",
    ">â€œOur method relies on an autonomous, pseudo-random procedure to select a small number of dimensions from a given feature space â€¦â€\n",
    "\n",
    " - Ho, Tin Kam. â€œThe random subspace method for constructing decision forests.â€ IEEE transactions on pattern analysis and machine intelligence 20.8 (1998): 832-844\n",
    "\n",
    "Just like the implementation shown in this project, modern (state of the art) random forest algorithms use random sampling of features at the node level. This approach allows the algorithm to learn the relationship between different features in a more diversified way. \n",
    "\n",
    "This means that each tree can work with the full set of features of the training data.\n",
    "\n",
    ">â€œâ€¦ random forest with random features is formed by selecting at random, at each node, a small group of input variables to split on.â€\n",
    "\n",
    " - Breiman, Leo. â€œRandom Forestsâ€ Machine learning 45.1 (2001): 5-32.\n",
    "\n",
    "### Bagging (or bootstrap aggregation)\n",
    "\n",
    "<img src=\"./images/bagging.png\" alt=\"k-fold\" width=\"400\"/>\n",
    "\n",
    "### Bootstrapping\n",
    "\n",
    "In addition to using a random subset of feature for each split, the trees in the Random Forest are also constructed using slightly different training data. This is achieved via a technique called bootstrapping. Bootstrapping uses sampling with replacement to construct a new version of the training dataset for every tree in the forest. Bootstrapping entails randomly sampling examples from the original dataset until a ratio size (`max_samples`) is reached. Sampling with replacement simply means that the same entries can be sampled multiple times from the original dataset.\n",
    "\n",
    "### Predicting a class (Aggregation)\n",
    "\n",
    "Once all the models in the forest have been constructed, we can start predicting labels for new data.\n",
    "The technique used by the Random Forest algorithm to predict a class label is pretty straightforward. The algorithm will run the prediction (exapleined in the previous section) for every tree in the forest. Majority voting is then applied on the list of predictions and the most frequent class value is then chosen as the return value.\n",
    "\n",
    "### A note on pruning\n",
    "\n",
    "Trees tend to be low-bias, high-variance models. This is the reason why, when building a single decision tree, employing pruning techniques is usually helpful in order to remove those branches that reflect noise and outliers in the training data.\n",
    "\n",
    "Random Forest uses feature randomness and bagging to \"de-correlate\" trees so that they are not overly similar. Predictions are then made considering all trees in the ensemble. This has the effect of mitigating this high variance and helps to avoid overfitting. Because of this reason, the benefits of emplying pruning techniques are usually greatly reduced when working with a Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRandomForestClassifier:\n",
    "    '''\n",
    "    Class used to model the random forest\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_estimators: int\n",
    "        Controls the nr of trees in the forest\n",
    "    bootstrap: bool\n",
    "        Decides whether bootstrapping is used for the trees input data\n",
    "        When set to False, the whole dataset is used for each tree in the forest\n",
    "    max_samples: float\n",
    "            The ratio size of the bootstrapping sample\n",
    "    criterion: str\n",
    "        Strategy used to calculate the impurity of a partition in a tree\n",
    "        Either 'gini' (for gini index) or 'entropy' (for information gain)\n",
    "    max_depth: int\n",
    "        Maximum total depth of the trees. Root node is at depth 1\n",
    "    min_impurity_decrease: float\n",
    "        Threshold for the reduction in impurity needed to split a partition in a tree\n",
    "        If the values is not met, the interested partition is turned into a leaf node\n",
    "    min_impurity_split: float\n",
    "        Impurity threshold below which a partition is turned into a leaf node \n",
    "    min_samples_split: int\n",
    "        Threshold for the nr of examples needed to split a partition in a tree\n",
    "        If the values is not met, the interested partition is turned into a leaf node\n",
    "    max_features: str\n",
    "        The number of features (randomly) considered at each split in a tree\n",
    "        'None' considers all the features in the training set\n",
    "        'sqrt' considers the square root of the total nr of features in the training set\n",
    "        'log2' considers the log base 2 of the total nr of features in the training set\n",
    "    min_samples_leaf: int\n",
    "        The minimum nr of entries required for a leaf node\n",
    "        A split candidate is considered only if it creates two child nodes that respect this parameter\n",
    "    '''\n",
    "    def __init__(self, *,\n",
    "                 n_estimators = 100,\n",
    "                 bootstrap = True,\n",
    "                 max_samples = 1.0,\n",
    "                 criterion = 'gini',\n",
    "                 max_depth = None, \n",
    "                 min_impurity_decrease = 0.0, \n",
    "                 min_impurity_split = None,\n",
    "                 min_samples_split = 2,\n",
    "                 max_features = 'sqrt',\n",
    "                 min_samples_leaf = 1):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth # Root depth is 0\n",
    "        self.bootstrap = bootstrap\n",
    "        self.max_samples = max_samples\n",
    "        self.n_estimators = n_estimators\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.min_impurity_split = min_impurity_split\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.n_classes = 0\n",
    "        self.forest = []\n",
    "\n",
    "    def fit(self, \n",
    "            X, \n",
    "            y):\n",
    "        '''\n",
    "        Starts the process of building a Random Forest\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: pandas.core.frame.DataFrame\n",
    "            The training examples\n",
    "        y: numpy.ndarray\n",
    "            The training class labels\n",
    "        '''\n",
    "        X, y = self.data_to_numpy(X, y)\n",
    "\n",
    "        # Flush old forest\n",
    "        if len(self.forest) > 0:\n",
    "            self.forest = []\n",
    "\n",
    "        self.n_classes = len(np.unique(y))\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            curr_tree = CustomDecisionTreeClassifier(criterion = self.criterion,\n",
    "                                                     max_depth = self.max_depth,\n",
    "                                                     min_impurity_decrease = self.min_impurity_decrease,\n",
    "                                                     min_impurity_split = self.min_impurity_split,\n",
    "                                                     min_samples_split = self.min_samples_split,\n",
    "                                                     max_features = self.max_features,\n",
    "                                                     min_samples_leaf = self.min_samples_leaf)\n",
    "\n",
    "            if self.bootstrap:\n",
    "                curr_X, curr_y = self.bootstrap_sample(X, y, self.max_samples)\n",
    "                curr_tree.fit(curr_X, curr_y)\n",
    "            else:\n",
    "                curr_tree.fit(X, y)\n",
    "\n",
    "            self.forest.append(curr_tree)\n",
    "\n",
    "    @staticmethod\n",
    "    def data_to_numpy(X,\n",
    "                      y = None):\n",
    "        '''\n",
    "        Changes input dataframes to numpy arrays\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: pandas.core.frame.DataFrame, numpy.ndarray\n",
    "            The training examples\n",
    "        y: pandas.core.frame.DataFrame, numpy.ndarray\n",
    "            The training class labels\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            The training examples converted to a numpy array\n",
    "        y: numpy.ndarray\n",
    "            The training class labels converted to a (flattened) numpy array\n",
    "        '''\n",
    "        if(type(X) != np.ndarray):\n",
    "            X = X.to_numpy()\n",
    "        if((y is not None) and (type(y) != np.ndarray)):\n",
    "            y = y.to_numpy()\n",
    "\n",
    "        if(y is not None):\n",
    "            return X, y.flatten()\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    @staticmethod\n",
    "    def bootstrap_sample(X, \n",
    "                         y, \n",
    "                         ratio = 1):\n",
    "        '''\n",
    "        Creates a new dataset (X_bootstrap, y_bootstrap) by randomly selecting examples from the original dataset (X, y) \n",
    "        until a ratio size is reached\n",
    "        The same examples from the original dataset may be sampled multiple times (sampling with replacement)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: pandas.core.frame.DataFrame\n",
    "            The training examples\n",
    "        y: numpy.ndarray\n",
    "            The training class labels\n",
    "        ratio: float\n",
    "            The ratio size of the sample\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        X_bootstrap: pandas.core.frame.DataFrame\n",
    "            New feature dataframe created from the sampled entries\n",
    "        y_bootstrap: numpy.ndarray\n",
    "            New class dataframe created from the sampled entries\n",
    "        '''\n",
    "        # Temporarily create a merged df to sample from\n",
    "        merged_df = np.append(X, np.vstack(y), axis=1)\n",
    "\n",
    "        samples_idx = np.random.choice(merged_df.shape[0], round(merged_df.shape[0]*ratio))\n",
    "        merged_df[samples_idx]\n",
    "\n",
    "        X_bootstrap = merged_df[:, :-1]\n",
    "        y_bootstrap = merged_df[:, -1]\n",
    "\n",
    "        return X_bootstrap, y_bootstrap\n",
    "\n",
    "    def predict(self, \n",
    "                X):\n",
    "        '''\n",
    "        Predicts the class value (label) for each input example\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.ndarray, pandas.core.frame.DataFrame\n",
    "            The examples to predict the class for\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        y: numpy.ndarray\n",
    "            The predicted class labels\n",
    "        '''\n",
    "        X = self.data_to_numpy(X)\n",
    "        y_forest = [] # Each index is the list of predictions from one tree\n",
    "\n",
    "        for tree in self.forest:\n",
    "            y_forest.append(tree.predict(X))\n",
    "\n",
    "        # Zip together prediction for same entry over different trees, extract most common class (majority voting)\n",
    "        return np.array(list(map(lambda i: Counter(i).most_common(1)[0][0], zip(*y_forest))))\n",
    "    \n",
    "    def predict_proba(self,\n",
    "                      X):\n",
    "        '''\n",
    "        Predicts the probability of each class value (label) for each input example\n",
    "        This probability is based on the output of the `predict` method for each tree in the forest for a\n",
    "        given example\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.ndarray, pandas.core.frame.DataFrame\n",
    "            The examples to predict the class for\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        y: numpy.ndarray\n",
    "            The probabilities for each class label\n",
    "        '''\n",
    "        X = self.data_to_numpy(X)\n",
    "        y_forest = [] # Each index is the list of predictions from one tree\n",
    "\n",
    "        for tree in self.forest:\n",
    "            y_forest.append(tree.predict(X))\n",
    "\n",
    "        # Zip together prediction for same entry over different trees, count the nr of predictions for each class\n",
    "        count_matrix = np.array(list(map(lambda i: Counter(i), zip(*y_forest))))\n",
    "        res = []\n",
    "\n",
    "        for i in range(len(count_matrix)):\n",
    "            temp = []\n",
    "            temp_total = sum(count_matrix[i].values())\n",
    "            for j in range(self.n_classes):\n",
    "                temp.append(count_matrix[i][j]/temp_total)\n",
    "            res.append(temp)\n",
    "\n",
    "        return np.array(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on regression\n",
    "\n",
    "Despite the fact that the two algorithms developed in this project are aimed at solving a classification task, it may still be useful to briefly explain how tree-based algorithms handle the prediction of continuous data (regression)\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on multi-class classification\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 3 - ESTABLISHING A BASELINE <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "## Section 3.1 - Preparing a scoring function <a class=\"anchor\" id=\"3.1\"></a>\n",
    "\n",
    "We are now ready to build our Decision Tree and Random Forest models starting from our training data. To aid in the evaluation of the developed algorithms, and to more easily allow a comparison between what has been developed and other pre-existing solutions, we will develop a new function that will compute a set of scoring metrics for each trained model.\n",
    "\n",
    "We define the concepts of true positive, true negative, false positive and false negative as follows:\n",
    "\n",
    "- **True positives**: positive tuples correctly labeled\n",
    "- **True negatives**: negative tuples correctly labeled \n",
    "- **False positives**: negative tuples incorrectly labeled (we should have predicted 0 but we predicted 1)\n",
    "- **False negatives**: positive tuples incorrectly labeled (we should have predicted 1 but we predicted 0)\n",
    "\n",
    "A confusion matrix is usually represented as follows:\n",
    "\n",
    "<img src=\"./images/confusion_matrix.png\" alt=\"k-fold\" width=\"400\"/>\n",
    "\n",
    "The `score_model` function will utilize the following metrics to evaluate our Decision Tree and Random Forest algorithms:\n",
    "\n",
    "- **Accuracy**: Measures how many class labels the model was able to correctly predict over the total number of prediction. An important aspect to keep in mind is that accuracy is not a very good metric if we are trying to evaluate a model when the classes are highly unbalanced. Accuracy is expressed as follows:\n",
    "\n",
    "$$ Accuracy = \\frac{\\textrm{Nr of correct predictions}}{\\textrm{Total nr of predictions}} = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "- **Precision**: For a given class, precision indicates how many time the model correctly predicted the class over the total number of times the class was predicted. For a class C, we express preicsion as follows:\n",
    "\n",
    "$$ Precision(C) = \\frac{\\textrm{Nr of correct C predictions}}{\\textrm{Total nr of C predictions}} = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "- **Recall**: For a given class, recall indicates the number of times the model correctly predicted the class over the true number of times the class appears. For a class C, we express recall as:\n",
    "\n",
    "$$ Recall(C) = \\frac{\\textrm{Nr of correct C predictions}}{\\textrm{Real nr of C occurrences}} = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "- **F1 Score**: Is the armonic mean of precision and recall. Intuitively, if both precision and recall are high, the F1 Score will be high. If one is high and one is low or if both are low, the F1 Score will be low. For a class C, we express the F1 Score as follows:\n",
    "\n",
    "$$ \\textrm{F1 Score(C)} = 2 * \\frac{Precision(C)*Recall(C)}{Precision(C)+Recall(C)} $$\n",
    "\n",
    "All these metrics are put together in the classification report. The report will contain the values for precision, recall and f1 score for all classes (0 and 1 in our case), a support column indicating the number of entries belonging to each class, the accuracy over the whole training set, the macro average (does not consider proportions) for all three metrics and the weighted average (does consider proportions) for all three metrics. \n",
    "\n",
    "Additionally, we will also print a confusion matrix that will visually show the percentage of correctly and incorrectly predicted values for each class.\n",
    "\n",
    "The score for each model is added to a new dataframe `scoring_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d3900e710063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscoring_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'library'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'algorithm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pruned'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'0_precision'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'0_recall'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'0_f1_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'0_support'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'1_precision'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'1_recall'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'1_f1_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'1_support'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'macro_avg_precision'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'macro_avg_recall'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'macro_avg_f1_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'macro_avg_support'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weighted_avg_precision'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weighted_avg_recall'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weighted_avg_f1_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weighted_avg_support'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m def score_model(*,\n\u001b[1;32m      4\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "scoring_df = pd.DataFrame(columns = ['library', 'algorithm', 'pruned', 'accuracy', 'fit_time', '0_precision', '0_recall', '0_f1_score', '0_support', '1_precision', '1_recall', '1_f1_score', '1_support', 'macro_avg_precision', 'macro_avg_recall', 'macro_avg_f1_score', 'macro_avg_support', 'weighted_avg_precision', 'weighted_avg_recall', 'weighted_avg_f1_score', 'weighted_avg_support'])\n",
    "\n",
    "def score_model(*,\n",
    "                model, \n",
    "                X,\n",
    "                y,\n",
    "                library_name,\n",
    "                algorithm_name,\n",
    "                fit_time,\n",
    "                pruned = False,\n",
    "                verbose = True,\n",
    "                add_to_df = True):\n",
    "    '''\n",
    "    Calculates and (optionally) prints the model score (for training and testing set) in terms of:\n",
    "        Accuracy\n",
    "        Precision\n",
    "        Recall\n",
    "        F1 score\n",
    "        Confusion Matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: \n",
    "        The trained model to score\n",
    "    X: numpy.ndarray, pandas.core.frame.DataFrame\n",
    "        The input examples\n",
    "    y: numpy.ndarray, pandas.core.frame.DataFrame\n",
    "        The input classes\n",
    "    library_name: str\n",
    "        Id of the library\n",
    "    algorithm_name: str\n",
    "        Id of the algorithm\n",
    "    fit_time: float\n",
    "        Time needed to build the model\n",
    "    verbose: bool\n",
    "        Controls whether the metrics are printed\n",
    "    '''\n",
    "    y_predict = model.predict_proba(X)\n",
    "    class_report = classification_report(y, np.around(y_predict[:, 1]), output_dict=True)\n",
    "\n",
    "    row = [library_name, \n",
    "           algorithm_name, \n",
    "           'Pruned' if pruned else 'Not Pruned',\n",
    "           class_report.get('accuracy'), fit_time]\n",
    "\n",
    "    for i in ['0', '1', 'macro avg', 'weighted avg']:\n",
    "        row += [class_report.get(i).get('precision'),\n",
    "                class_report.get(i).get('recall'),\n",
    "                class_report.get(i).get('f1-score'),\n",
    "                class_report.get(i).get('support')]\n",
    "\n",
    "    if add_to_df:\n",
    "        scoring_df.loc[len(scoring_df)] = row\n",
    "\n",
    "    if(verbose):\n",
    "        print(\"\\n\")\n",
    "        print(classification_report(y, np.around(y_predict[:, 1])))\n",
    "        plot_confusion_matrix(y, y_predict)\n",
    "\n",
    "def plot_confusion_matrix(y, y_predict):\n",
    "    cm = confusion_matrix(y, np.around(y_predict[:, 1]), normalize = 'true')\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, fmt='g', ax=ax);\n",
    "\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Real\") \n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.2 - Preparing an hyperparameter tuning function <a class=\"anchor\" id=\"3.2\"></a>\n",
    "\n",
    "In order to improve the performance of our model and fully utilize the hyperparameters that we have implemented in our custom algorithms, we will use a technique called grid search. GridSearchCV takes as input a `param_grid` (which is a dictionary containing a list of hyperparameters together with the values they can take), an estimator (i.e. the classifier that we want to tune), the training set, a scoring function and the nr of folds we want to use in our K-fold cross validation. The algorithm tries all possible hyperparameters combinations from the values specified in our `param_grid` and returns the estimator that yielded the best result based on the chosen scoring metric.\n",
    "\n",
    "As already mentioned, GridSearchCV uses K-fold cross validation to evaluate each model. Using this approach the dataset is split into K subsets (folds). The model is then built on K-1 (training) folds and scored on the last one. The process is repeated until all folds have been used for testing. The final score of the given model is an average across all iterations.\n",
    "\n",
    "<img src=\"./images/k-fold-cross-validation.png\" alt=\"k-fold\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_estimator(*,\n",
    "                       _param_grid, \n",
    "                       _estimator, \n",
    "                       X, \n",
    "                       y,\n",
    "                       _cv = 3,\n",
    "                       _scoring = 'f1',\n",
    "                       verbose = True):\n",
    "    '''\n",
    "    Performs a grid search on the estimator using the provided parameters grid.\n",
    "    Optionally, prints the parameters for the best estimator\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    _param_grid: dict\n",
    "        The hyperparameters grid\n",
    "    _estimator: \n",
    "        The estimator on which to run the grid search\n",
    "    X: numpy.ndarray, pandas.core.frame.DataFrame\n",
    "        The input examples\n",
    "    y: numpy.ndarray, pandas.core.frame.DataFrame\n",
    "        The input classes\n",
    "    _cv: int\n",
    "        Nr of fold used for cross validation\n",
    "    _scoring: str\n",
    "        Strategy used to evaluate the model\n",
    "    verbose: bool\n",
    "        Controls whether the parameters for the best estimator are printed\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    best_params: dict\n",
    "        Hyperparameters of the best estimator\n",
    "    '''\n",
    "\n",
    "    grid_search = GridSearchCV(estimator = _estimator, \n",
    "                               param_grid = _param_grid, \n",
    "                               cv = _cv,\n",
    "                               scoring = _scoring, \n",
    "                               n_jobs = -1)\n",
    "    grid_search.fit(X, y)\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    if(verbose):\n",
    "        print('Parameters of the best estimator:')\n",
    "        for i in best_params:\n",
    "            print (i,':',best_params[i])\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.3 - Training the models <a class=\"anchor\" id=\"3.3\"></a>\n",
    "\n",
    "### Sklearn Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''start_time = time.time()\n",
    "sklearn_tree = DecisionTreeClassifier(random_state=42)\n",
    "sklearn_tree.fit(X_train_SMOTE, y_train_SMOTE.to_numpy().flatten())\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "score_model(model = sklearn_tree, \n",
    "            X = X_test, \n",
    "            y = y_test, \n",
    "            library_name = 'Sklearn', \n",
    "            algorithm_name = 'CART (Not tuned)', \n",
    "            fit_time = end_time)'''\n",
    "\n",
    "# Tuned Decision Tree\n",
    "sklearn_tree_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': range(1,25),\n",
    "    'min_impurity_decrease': [0.0, 0.1, 0.2],\n",
    "    'min_samples_leaf': range(1,10),\n",
    "    'min_samples_split': range(2,10)\n",
    "}\n",
    "\n",
    "sklearn_tree_params = get_best_estimator(_param_grid = sklearn_tree_grid,\n",
    "                                         _estimator = DecisionTreeClassifier(random_state=42),\n",
    "                                         X = X_train_SMOTE,\n",
    "                                         y = y_train_SMOTE.to_numpy().flatten())\n",
    "\n",
    "start_time = time.time()\n",
    "sklearn_tree = DecisionTreeClassifier(**sklearn_tree_params, random_state=42)\n",
    "sklearn_tree.fit(X_train_SMOTE, y_train_SMOTE.to_numpy().flatten())\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "score_model(model = sklearn_tree, \n",
    "            X = X_test, \n",
    "            y = y_test, \n",
    "            library_name = 'Sklearn', \n",
    "            algorithm_name = 'CART',\n",
    "            fit_time = end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''start_time = time.time()\n",
    "sklearn_forest = RandomForestClassifier(random_state=42)\n",
    "sklearn_forest.fit(X_train_SMOTE, y_train_SMOTE.to_numpy().flatten())\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "score_model(model = sklearn_forest, \n",
    "            X = X_test, \n",
    "            y = y_test, \n",
    "            library_name = 'Sklearn', \n",
    "            algorithm_name = 'Random Forest (Not tuned)', \n",
    "            fit_time = end_time)'''\n",
    "\n",
    "# Tuned Random Forest\n",
    "sklearn_forest_grid = {\n",
    "    'max_features': range(4,21),\n",
    "    'n_estimators': [100, 200, 300, 400]\n",
    "}\n",
    "\n",
    "sklearn_forest_params = get_best_estimator(_param_grid = sklearn_forest_grid,\n",
    "                                           _estimator = RandomForestClassifier(random_state=42),\n",
    "                                           X = X_train_SMOTE,\n",
    "                                           y = y_train_SMOTE.to_numpy().flatten(),\n",
    "                                           _scoring='roc_auc')\n",
    "\n",
    "start_time = time.time()\n",
    "sklearn_forest = RandomForestClassifier(**sklearn_forest_params, random_state=42)\n",
    "sklearn_forest.fit(X_train_SMOTE, y_train_SMOTE.to_numpy().flatten())\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "score_model(model = sklearn_forest, \n",
    "            X = X_test, \n",
    "            y = y_test, \n",
    "            library_name = 'Sklearn', \n",
    "            algorithm_name = 'Random Forest', \n",
    "            fit_time = end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 4 - MODELLING USING OUR CUSTOM IMPLEMENTATION <a class=\"anchor\" id=\"4\"></a>\n",
    "\n",
    "### Custom Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''start_time = time.time()\n",
    "custom_tree = CustomDecisionTreeClassifier()\n",
    "custom_tree.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "score_model(model = custom_tree, \n",
    "            X = X_test, \n",
    "            y = y_test, \n",
    "            library_name = 'Custom', \n",
    "            algorithm_name = 'CART (Not tuned)', \n",
    "            fit_time = end_time)'''\n",
    "            \n",
    "# Tuned Decision Tree\n",
    "start_time = time.time()\n",
    "custom_tree = CustomDecisionTreeClassifier(**sklearn_tree_params)\n",
    "custom_tree.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "score_model(model = custom_tree, \n",
    "            X = X_test, \n",
    "            y = y_test, \n",
    "            library_name = 'Custom', \n",
    "            algorithm_name = 'CART', \n",
    "            fit_time = end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Random forest classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''start_time = time.time()\n",
    "custom_forest = CustomRandomForestClassifier()\n",
    "custom_forest.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "score_model(model = custom_forest, \n",
    "            X = X_test, \n",
    "            y = y_test, \n",
    "            library_name = 'Custom', \n",
    "            algorithm_name = 'Random Forest (Not tuned)', \n",
    "            fit_time = end_time)'''\n",
    "\n",
    "# Tuned Random Forest\n",
    "start_time = time.time()\n",
    "custom_forest = CustomRandomForestClassifier(**sklearn_forest_params)\n",
    "custom_forest.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "score_model(model = custom_forest, \n",
    "            X = X_test, \n",
    "            y = y_test, \n",
    "            library_name = 'Custom', \n",
    "            algorithm_name = 'Random Forest', \n",
    "            fit_time = end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 5 - COMPARING RESULTS <a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    "Now that we have finished building our models, we can finally compare the results of our custom implementations of the Decision Tree and Random Forest algorithms to the ones obtained by using the Sklearn implementation. As a first step, we will show the content of the `scoring_df` dataset containing all of our recorded metrics. Then, we will create a set of visual representations to show the difference in performance between our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_algorithm_comparison(scoring_df, plot_metrics, fig_x = 25, fig_y = 10, curr_hue='library'):\n",
    "    fig, ax = plt.subplots(len(plot_metrics), len(plot_metrics[0]), figsize=(fig_x, fig_y), sharey=True)\n",
    "\n",
    "    for row in range(len(plot_metrics)):\n",
    "        for column in range(len(plot_metrics[0])):\n",
    "            sns.barplot(y='algorithm', \n",
    "                x=plot_metrics[row][column], \n",
    "                hue=curr_hue,\n",
    "                alpha=.75,\n",
    "                palette=['#d11313', '#3697b5'],\n",
    "                data=scoring_df,\n",
    "                ax=ax[row][column]).set_title(plot_metrics[row][column])\n",
    "\n",
    "            ax[row,column].set_ylabel('')\n",
    "            ax[row,column].set_ylabel('')\n",
    "            ax[row,column].set_xlabel('')\n",
    "            ax[row,column].set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_algorithm_comparison(scoring_df,\n",
    "                          [['accuracy', 'fit_time'], \n",
    "                          ['0_precision', '1_precision'],\n",
    "                          ['0_recall', '1_recall'],\n",
    "                          ['0_f1_score', '1_f1_score']],\n",
    "                          fig_x = 20,\n",
    "                          fig_y = 20)\n",
    "\n",
    "plot_algorithm_comparison(scoring_df,\n",
    "                          [['macro_avg_precision', 'macro_avg_recall', 'macro_avg_f1_score'],\n",
    "                          ['weighted_avg_precision', 'weighted_avg_recall', 'weighted_avg_f1_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plotted metrics the results of our custom implementations of the CART and Random Forest algorithms achieve comparable results across the board when put side-by-side with the implementations offered by Sklearn. The one aspect where our implementation seems to fall behind is fit time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 6 - EXTRA <a class=\"anchor\" id=\"6\"></a>\n",
    "\n",
    "## Section 6.1 - Pruning with Sklearn implementation of CCP <a class=\"anchor\" id=\"6.1\"></a>\n",
    "\n",
    "The cost complexity R_alpha(T) of a tree T is defined as follows:\n",
    "\n",
    "$$ R _{\\alpha} (T) = R(T) + \\alpha |T| $$\n",
    "\n",
    "Where |T| is the number of leaf nodes of T and R(T) is the total (training) error (misclassification rate) of the leaf nodes. Often times trees have a very low R(T) because they are overgrown (i.e. they overfit the training data). Alpha is called complexity parameter and it controls the nr of leaf nodes in the tree. The objective of CCP is to minimize R_alpha(T). The result of CCP is usually a model that is slightly less accurate on training data, but more capable to generalize on new data.\n",
    "\n",
    "For a single node t, with R(t) indicating the training error at the given node, we have that:\n",
    "\n",
    "$$ R _{\\alpha} (t) = R(t) + \\alpha $$\n",
    "\n",
    "If we now take t as the root of a sub-tree T_t, we have its training error R(T_t) defined as the sum of the training error of all its leaf nodes L:\n",
    "\n",
    "$$ R(T _{t}) = \\sum \\limits _{i = 1} ^L R(t _{i}) $$\n",
    "\n",
    "The cost complexity of T_t can be calculated as shown in the first equation:\n",
    "\n",
    "$$ R _{\\alpha} (T _{t}) = R(T _{t}) + \\alpha |T _{t}| $$\n",
    "\n",
    "Now, if the training error at a node t is equal to the training error for the sub_tree T_t (which is rooted in t):\n",
    "\n",
    "$$ R(t) = R(T _{t}) $$\n",
    "\n",
    "We can say that the split made at t is redundant and we can prune the sub-tree T_t. If we generalize this concept to the cost complexity measure:\n",
    "\n",
    "$$ R _{\\alpha} (t) = R _{\\alpha} (T _{t}) $$\n",
    "\n",
    "$$ R(t) + \\alpha = R(T _{t}) + \\alpha |T _{t}| $$\n",
    "\n",
    "And solve for alpha, we obtain:\n",
    "\n",
    "$$ \\alpha _{effective} (t) = \\frac{R(t) - R(T _{t})}{|T| - 1} $$\n",
    "\n",
    "This values of alpha, called effective alpha, is computed for each inner node. Non-terminal nodes with the smallest effective alphas (which are the weakest links) are pruned until only the root nodes remains.\n",
    "\n",
    "With alpha set to 0, the unpruned tree is returned. As alpha approaches infinity, only the root node is returned.\n",
    "\n",
    "Setting the value of `ccp_alpha` (threshold for the weakest link) of the `DecisionTreeClassifier` will cause the tree to be pruned until the trees smallest effective alpha reaches `ccp_alpha`. The method `cost_complexity_pruning_path` performs the pruning process and returns the set of effective alphas (and impurities) of a given tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = DecisionTreeClassifier(**sklearn_tree_params, random_state=42).cost_complexity_pruning_path(X_train_SMOTE, y_train_SMOTE)\n",
    "\n",
    "# CCP alphas of subtrees + Sum of impurities of each subtree leaves\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities \n",
    "\n",
    "# Test the decision tree with all CCP alphas\n",
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(**sklearn_tree_params, ccp_alpha=ccp_alpha, random_state=42)\n",
    "    clf.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "    clfs.append(clf)\n",
    "\n",
    "clfs = clfs[:-1]\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "node_counts = [clf.tree_.node_count for clf in clfs]\n",
    "depth = [clf.tree_.max_depth for clf in clfs]\n",
    "\n",
    "fig, ax = plt.subplots(4, 1, figsize=(8, 20))\n",
    "\n",
    "sns.lineplot(x=ccp_alphas, y=impurities[:-1], drawstyle='steps-pre', ax=ax[0])\n",
    "ax[0].set(xlabel='Alpha', ylabel='Total impurity of leaves', title='Training impurity vs alpha')\n",
    "\n",
    "sns.lineplot(x=ccp_alphas, y=node_counts,  drawstyle='steps-pre', ax=ax[1])\n",
    "ax[1].set(xlabel='Alpha', ylabel='Nr of nodes', title='Nr of nodes vs alpha')\n",
    "\n",
    "sns.lineplot(x=ccp_alphas, y=depth,  drawstyle='steps-pre', ax=ax[2])\n",
    "ax[2].set(xlabel='Alpha', ylabel='Tree depth', title='Alpha vs depth')\n",
    "\n",
    "train_scores = [accuracy_score(y_train_SMOTE, clf.predict(X_train_SMOTE)) for clf in clfs]\n",
    "test_scores = [accuracy_score(y_test, clf.predict(X_test)) for clf in clfs]\n",
    "\n",
    "sns.lineplot(x=ccp_alphas, y=train_scores, drawstyle='steps-pre', label='Train', ax=ax[3])\n",
    "sns.lineplot(x=ccp_alphas, y=test_scores,  drawstyle='steps-pre', label='Test', ax=ax[3])\n",
    "ax[3].set(xlabel='Alpha', ylabel='Accuracy', title='Accuracy vs alpha')\n",
    "ax[3].legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now apply one of the optimal values of `ccp_alpha` to both the `DecisionTreeClassifier` and the `RandomForestClassifier`. As stated in one of the previous sections, cost complexity pruning will mostly benefit our decision tree model, leading only to minor to non-existent benefits to the random forest classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pruned_sklearn_tree = DecisionTreeClassifier(**sklearn_tree_params, ccp_alpha = 0.011, random_state=42)\n",
    "pruned_sklearn_tree.fit(X_train_SMOTE, y_train_SMOTE.to_numpy().flatten())\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "score_model(model=pruned_sklearn_tree, \n",
    "            X=X_test,\n",
    "            y=y_test,\n",
    "            library_name='Sklearn',\n",
    "            algorithm_name='CART',\n",
    "            fit_time=end_time,\n",
    "            pruned=True,\n",
    "            verbose = False)\n",
    "\n",
    "start_time = time.time()\n",
    "pruned_sklearn_forest = RandomForestClassifier(**sklearn_forest_params, ccp_alpha = 0.011, random_state=42)\n",
    "pruned_sklearn_forest.fit(X_train_SMOTE, y_train_SMOTE.to_numpy().flatten())\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "score_model(model = pruned_sklearn_forest, \n",
    "            X = X_test, \n",
    "            y = y_test, \n",
    "            library_name = 'Sklearn', \n",
    "            algorithm_name = 'Random Forest', \n",
    "            fit_time = end_time,\n",
    "            pruned=True,\n",
    "            verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_algorithm_comparison(scoring_df[scoring_df['library'] == 'Sklearn'],\n",
    "                          [['accuracy', 'fit_time'],\n",
    "                          ['0_precision', '1_precision'],\n",
    "                          ['0_recall', '1_recall'],\n",
    "                          ['0_f1_score', '1_f1_score']],\n",
    "                          fig_x = 20,\n",
    "                          fig_y = 20,\n",
    "                          curr_hue='pruned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These metrics confirmed our expectations regarding the effectnivness of CCP. For the `DecisionTreeClassifier` we have seen improvements for both classes across all metrics. On the other hand, the application of this technique on the `RandomForestClassifier` only lead to fairly inconsistent improvements in relation to the non-pruned model. This observation is additional proof that feature randomness and bagging are already addressing the high-variance typical of decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6.2 - Visual representation of pruned & non-pruned Decision Tree\n",
    "\n",
    "As a last step before our concluding statement, we will plot both the pruned and non-pruned version of the Sklearn-based `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=300)\n",
    "tree.plot_tree(sklearn_tree)\n",
    "fig.savefig('images/non-pruned-tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=300)\n",
    "tree.plot_tree(pruned_sklearn_tree)\n",
    "fig.savefig('images/pruned-tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 7 - CONCLUSIONS <a class=\"anchor\" id=\"7\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
